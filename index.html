<!DOCTYPE html>
<html lang="en">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <link href='https://fonts.googleapis.com/css?family=Noto Sans' rel='stylesheet'>
    <link href='https://fonts.googleapis.com/css?family=Indie Flower' rel='stylesheet'>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>HaLo-NeRF: Text-Driven Neural 3D Localization in the Wild</title>
<!--    <link rel="icon" href="../pics/wis_logo.jpg">-->
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto Sans">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto Sans">
    <link href="./style_supp.css" rel="stylesheet" type="text/css">
</head>
<script type="text/javascript">
    src=hover.js;
</script>
<body>
<head>
  <meta charset="utf-8">
  <meta name="description" content="Scene flow estimation from point clouds based on pure correspondence learning and direct refinement optimization.">
  <meta name="keywords" content="SCOOP, Scene Flow, Point Clouds, Self-Supervised, Correspondence, Optimization, CVPR 2023">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="icon" href="./assets/icon_wo_bg.png">

  <title>HaLo-NeRF: Text-Driven Neural 3D Localization in the Wild</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-Q6JSKPD63W"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-Q6JSKPD63W');
  </script>
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<style>
.author-name-small {
  font-size: 19px; /* You can adjust the font size to your preference */
}
</style>

<style>
    /* Add some styles for better visualization */
    .controls {
      text-align: center;
      margin-top: 10px;
    }

    .dot {
      height: 15px;
      width: 15px;
      margin: 0 5px;
      background-color: #bbb;
      border-radius: 50%;
      display: inline-block;
      transition: background-color 0.6s ease;
      cursor: pointer;
    }

    .arrow {
      font-size: 24px;
      cursor: pointer;
    }
	.video-description {
    margin-top: 50px;
    text-align: center;
    display: flex;
    flex-direction: column;
    align-items: center;
	font-size: 26px; /* Adjust the font size as needed */
  }

  .video-description h4,
  .video-description p {
    writing-mode: horizontal-tb !important;
    transform: rotate(0deg) !important;
  }
  
   .halonerf-text {
    font-weight: bold;
  }
  </style>

<body>
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">HaLo-NeRF<span style='font-size:50px;'>&#128519;</span> <br> Text-Driven Neural 3D Localization in the Wild</h1>
          <!--<h2 class="title is-3 publication-conference">Eurographics 2024</h2>-->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <div class="author-portrait">
                <img src="./assets/chen.png" class="rgb preload" alt="chen">
              </div>
              <a href="https://www.linkedin.com/in/chen-dudai-108a72136/?originalSubdomain=il" class="author-name-small">Chen Dudai</a><sup>1</sup>
            </span>
            <span class="author-block">
              <div class="author-portrait">
                <img src="./assets/morris.png" class="rgb preload" alt="morris">
              </div>
              <a href="https://morrisalp.github.io/" class="author-name-small">Morris Alper*</a><sup>1</sup>
            </span>
            <span class="author-block">
              <div class="author-portrait">
                <img src="./assets/hana.png" class="rgb preload" alt="hana">
              </div>
              <a href="https://www.linkedin.com/in/hanabezalel/?originalSubdomain=il" class="author-name-small">Hana Bezalel</a><sup>1</sup>
            </span>
            <span class="author-block">
              <div class="author-portrait">
                <img src="./assets/rana.png" class="rgb preload" alt="rana">
              </div>
              <a href="http://people.cs.uchicago.edu/~ranahanocka/" class="author-name-small">Rana Hanocka</a><sup>2</sup>
            </span>
            <span class="author-block">
              <div class="author-portrait">
                <img src="./assets/itai.png" class="rgb preload" alt="itai">
              </div>
              <a href="https://itailang.github.io/" class="author-name-small">Itai Lang</a><sup>2</sup>
            </span>
			<span class="author-block">
              <div class="author-portrait">
                <img src="./assets/hadar.png" class="rgb preload" alt="hadar">
              </div>
              <a href="https://www.elor.sites.tau.ac.il/" class="author-name-small">Hadar Averbuch-Elor</a><sup>1</sup>
            </span>
          </div>
		  
		  		  
		   <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>*Equal contribution</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup>Tel Aviv University
            </span>
            <span class="author-block">
              &nbsp;&nbsp;&nbsp;<sup>2</sup>The University of Chicago
            </span>
          </div>
		  



          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Paper Link -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
			  <!-- Code Link -->
              <span class="link-block">
                <a href="https://github.com/TAU-VAILab/HaLo-NeRF"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- supp Link -->
              <span class="link-block">
                <a href="https://github.com/TAU-VAILab/HaLo-NeRF/blob/site/supplementary.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                
                  <span>Supplementary</span>
                </a>
              </span>
              <!-- dataset Link -->
              <span class="link-block">
                <a href="https://drive.google.com/drive/u/1/folders/10uweGJ2JURQsmwS7pE4HZayi-m7nRRId"
                   class="external-link button is-normal is-rounded is-dark">
               
                  <span>Dataset</span>
                </a>
              </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<center>
	<video  align="center"  width="768" height="576"  autoplay loop muted>
		<source src="./assets/video.mp4" type="video/mp4">
		Your browser does not support the video tag.
	</video>
<center>



<section class="section">
  <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
        <div class="content has-text-justified">

<h2 class="title is-5 publication-conference">
TL;DR We learn a semantic localization field for textual descriptions over collections of in-the-wild images depicting a large-scale scene.
</h2>
      </div>
	          <div class="content has-text-justified">

	              As illustrated for St. Paul's Cathedral above, our approach enables generating novel views with controlled appearances of these semantic regions of interest.
	      </div>

      </div>
	  
      </div>

  </div>

<br>

 <div class="container is-max-desktop">
        <div class="column is-four-fifths">
        <div class="content has-text-justified">
		<table class="result-table" width="100%" align="center">
		<tbody>
                <tr>
                    <th colspan="2" width="45%" class="prompt_title"></th>
                </tr>
				<tr>
                </tr>	
		</tbody>
	</table>
      </div>
	        </div>
 </div>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The emergence of neural radiance fields has revolutionized our means for digitally exploring large-scale tourist landmarks,
			enabling the synthesis of photorealistic images of varying illumination, time, and season. However, these neural representations
			mainly provide low-level appearance information, which lacks a higher-level semantic understanding of these scenes. 
			In this work, we present HaLo-NeRF, a 3D localization system that connects neural representations of scenes depicting such large-
			scale landmarks with text describing a semantic region within the scene. Our method harnesses the power of SOTA vision-and-
			language models to connect Internet imagery with scene semantics. Although these models display an excellent understanding
			of broad visual semantics, they lack expert knowledge of specific domains, such as culturally significant architecture, which is a
			prerequisite for localizing these concepts in space. To bolster such models with fine-grained knowledge, we leverage large-scale
			Internet data containing images of similar landmarks along with weakly-related textual information. Our approach is built upon
			the premise that images physically grounded in space can provide a powerful supervision signal for localizing new concepts,
			whose semantics may be unlocked from Internet textual metadata with large language models. We use correspondences between
			views of scenes to bootstrap spatial understanding of these semantics, providing guidance for 3D-compatible segmentation that
			ultimately lifts to a volumetric scene representation. To evaluate our method, we present a new benchmark dataset containing
			large-scale scenes with ground-truth segmentations for multiple semantic concepts. Our results show that HaLo-NeRF can
			accurately localize a variety of semantic concepts related to architectural landmarks, surpassing the results of other 3D models
			as well as strong 2D segmentation baseline.
          </p>
        </div>
      </div>
    </div>
	
	
	 <div class="column is-four-fifths">
        <div class="content has-text-justified">
		<table class="result-table" width="100%" align="center">
		<tbody>
                <tr>
                    <th colspan="2" width="45%" class="prompt_title"></th>
                </tr>
				<tr>
                </tr>	
		</tbody>
	</table>
      </div>
 </div>
		<br><br>	
	
	<h2 class="title is-3">Neural 3D Localization Results of our Method</h2>
	<div class="content has-text-centered">
        <img id="results density" width="80%" src="./assets/results.png" alt="Density">
		
    </div>
	 <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
	We demonstrate sample
results from our HolyScenes benchmark (depicting Milan Cathedral
on top and Badshahi Mosque on bottom), visualizing segmentation
maps over input images from test landmarks. As illustrated above,
HaLo-NeRF successfully segments regions corresponding to a given
text input. Moreover, our method can differentiate between concepts
with similar semantic descriptions, such as Portal and Window
	        </div>
      </div>
    </div>


 <div class="column is-four-fifths">
        <div class="content has-text-justified">
		<table class="result-table" width="100%" align="center">
		<tbody>
                <tr>
                    <th colspan="2" width="45%" class="prompt_title"></th>
                </tr>
				<tr>
                </tr>	
		</tbody>
	</table>
      </div>
 </div>


<section class="section">
   <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
		        <div class="content has-text-centered">
          <img id="system" width="100%" src="./assets/system.png" alt="System">
        </div>
          <p>
            Our goal is to perform text-driven neural 3D localization for landmark scenes captured by collections of Internet photos.
			In other words, given this collection of images and a text prompt describing a semantic concept in the scene, we would like to know where it is located in 3D space.
			These images are in the wild, meaning that they may be taken in different seasons, time of day, viewpoints, and distances from the landmark, and may include transient occlusions. 
          </p>
          <p>
           In order to localize unique architectural features landmarks in 3D space, we leverage the power modern foundation models for visual and textual understanding.
		   Despite progress in general multimodal understanding, modern VLMs struggle to localize fine-grained semantic concepts on architectural landmarks, as we show extensively in our results.
		   The architectural domain uses a specialized vocabulary, with terms being rare in general usage.
          </p>
		  </p>
		  To address these challenges, we design a three-stage system: (a) We extract semantic pseudo-labels from noisy Internet image metadata using a large
			language model (LLM). (b) We use these pseudo-labels and correspondences between scene views to learn image-level and pixel-level
			semantics. In particular, we fine-tune an image segmentation model (CLIPSegFT) using multi-view supervision—where zoomed-in views and
			their associated pseudo-labels (such as image on the left associated with the term “tympanum”) provide a supervision signal for zoomed-out
			views. (c) We then lift this semantic understanding to learn volumetric probabilities over new, unseen landmarks (such as the St. Paul’s
			Cathedral depicted on the right), allowing for rendering views of the segmented scene with controlled viewpoints and illumination settings.
		  </p>
        </div>

	
      </div>
    </div>
  </div>

<br>
 <div class="column is-four-fifths">
        <div class="content has-text-justified">
		<table class="result-table" width="100%" align="center">
		<tbody>
                <tr>
                    <th colspan="2" width="45%" class="prompt_title"></th>
                </tr>
				<tr>
                </tr>	
		</tbody>
	</table>
      </div>
 </div>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="column is-full-width">
          <h2 class="title is-3">Interactive Visualizations</h2>
          <div class="content has-text-justified">
            <div class="content has-text-justified">
              <p>
In addition, we show below interactive visualizations, comparing HaLo-NeRF (left) with the Baseline model (right),
 which uses the CLIPSeg model without finetuning. Both videos show the same temporal sequence of RGB renderings, varying only in the probabilities depicted (taken either from our model or the baseline).
 Note that once zoomed-in, we turn off the probabilities for both models, allowing to better view the target semantic region.
 The target text prompt is written above each video, with the name of the landmark on the right. As illustrated below, our model yields significantly cleaner probabilities that better localize the semantic regions, particularly for unique concepts that are less common outside of the domain of architectural landmarks. Note that once zoomed-in, we turn off the probabilities for both models, allowing to better view the target semantic region. We also visualize the zoomed-in region with multiple appearance (for our model, keeping the appearance of the baseline model fixed). Results over additional prompts and landmarks from the HolyScenes benchmark are illustrated in the main paper.
              </p>
            </div>
          </div>

          <div class="video-description">
            <h4 id="video-description-title"></h4>
            <p id="video-description-content"></p>
          </div>
		  
          <div id="video-slideshow" class="video-container1">
            <!-- Videos will be displayed here -->
          </div>


				<!-- Text under the videos -->
		<div class="video-text">
		  <div class="halonerf-text">HaLo-NeRF   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp  Baseline</div>
		</div>
		
          <div class="controls">
            <span class="arrow" onclick="prevVideo()">&#10094;</span>
            <span class="dot" onclick="currentVideo(0)"></span>
            <span class="dot" onclick="currentVideo(1)"></span>
            <span class="dot" onclick="currentVideo(2)"></span>
			<span class="dot" onclick="currentVideo(3)"></span>
            <!-- Add more dots as needed -->
            <span class="arrow" onclick="nextVideo()">&#10095;</span>
          </div>

			

          <script>
            const videos = [
              {
                url: "./assets/portals_notre_dame.mp4",
                title: '"Portals" (a grand entrance to a cathedral),  Notre-Dame Cathedral',
                content: ''
              },
              {
                url: "./assets/blue_mosque_minarets.mp4",
                title: '"Minarets" (a tall slender tower of a mosque),      Blue Mosque',
                content: ''
              },
              {
                url: "./assets/pediment_st_paul.mp4",
                title: '"Pediment" (a triangular part at the top of the front of a building),       St. Paul\'s Cathedral',
                content: ''
              },
			  {
                 url: "./assets/windows_milano.mp4",
                title: '"Windows",       Milan Cathedral',
                content: ''
              }
			
              // Add more video objects as needed
            ];

            let currentVideoIndex = 0;

            function showVideo(index) {
              const videoSlideshow = document.getElementById("video-slideshow");
              videoSlideshow.innerHTML = `
                <video autoplay width="100%" class="result-video1"  autoplay loop muted>
                  <source src="${videos[index].url}" type="video/mp4">
                </video>
              `;
              updateDots(index);
              updateVideoDescription(index);
            }

            function updateDots(index) {
              const dots = document.querySelectorAll('.dot');
              dots.forEach(dot => dot.style.backgroundColor = '#bbb');
              dots[index].style.backgroundColor = '#717171';
            }

            function updateVideoDescription(index) {
              const titleElement = document.getElementById('video-description-title');
              const contentElement = document.getElementById('video-description-content');
              titleElement.innerText = videos[index].title;
              contentElement.innerText = videos[index].content;
            }

            function currentVideo(index) {
              currentVideoIndex = index;
              showVideo(currentVideoIndex);
            }

            function prevVideo() {
              currentVideoIndex = (currentVideoIndex - 1 + videos.length) % videos.length;
              showVideo(currentVideoIndex);
            }

            function nextVideo() {
              currentVideoIndex = (currentVideoIndex + 1) % videos.length;
              showVideo(currentVideoIndex);
            }

            showVideo(currentVideoIndex);
          </script>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Citation</h2>
    <pre><code>
    </code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2211.14020.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/itailang">
        <i class="fab fa-github"></i>
      </a>
      <a class="icon-link" href="https://scholar.google.com/citations?user=q0bBhtsAAAAJ">
        <i class="ai ai-google-scholar"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            This website is adapted from the <a href="https://itailang.github.io/SCOOP/">SCOOP</a> website. We thank
            <a href="https://itailang.github.io/">Itai Lang</a> for sharing his
            <a href="https://github.com/itailang/SCOOP/tree/site">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>

</html>